# quantization 
**ğŸ§  MNIST Dataset Quantization with Keras**

This project demonstrates the application of Deep Learning Model Optimization techniques such as Pruning and Quantization on the popular MNIST handwritten digits dataset. Built using TensorFlow/Keras, it showcases how to achieve significant model compression while maintaining (or even improving) performance, with detailed benchmarking of accuracy, model size, and inference speed.

**ğŸ“ Project Structure**


Quantization.ipynb: Main Jupyter notebook containing:

Baseline model training on MNIST

Post-training quantization (PQT)

Quantization-aware training (QAT)

Model evaluation and comparison

**ğŸš€ Techniques Covered**

âœ… Baseline model development and training

âœ… Post-training quantization to INT8 (PQT)

âœ… Quantization-aware training (QAT) for better accuracy under quantization constraints

âœ… Model size, latency, and load time comparisons

âœ… Accuracy and loss evaluation across different compression techniques

**ğŸ“Š Sample Results**

Model Type	File Size	Loss	Accuracy	Load Time (s)

Baseline Model	522 KB	0.1653	95.31%	0.0897

PQT Compressed + Quantized	45 KB	0.0903	97.31%	0.0359

QAT Quantized Model	92 KB	0.9505	74.49%	0.0006

**ğŸ”— References:**

Keras Model Optimization Documentation:  https://www.tensorflow.org/model_optimization

Netron - Visualize ML Models :https://netron.app/
